{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BH_FILE = \"bh.dat\"\n",
    "TX_FILE = \"tx.dat\"\n",
    "TXIN_FILE = \"txin.dat\"\n",
    "TXOUT_FILE = \"txout.dat\"\n",
    "\n",
    "LINE_BATCH_SIZE = 500\n",
    "\n",
    "\n",
    "def load_dat(path, as_ndarray=False, start=0, n_lines=\"all\"):\n",
    "    \"\"\"\n",
    "     Read 'n_lines' lines starting from line 'start' from the file 'path'.\n",
    "     Return numpy array or list of tuples.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    with open(path, \"r\") as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            if i >= start and (n_lines == \"all\" or n_lines > 0):\n",
    "                result_list.append(tuple(line.split()))\n",
    "                if type(n_lines) is not str:\n",
    "                    n_lines -= 1\n",
    "    return np.array(result_list) if as_ndarray else result_list\n",
    "\n",
    "\n",
    "def prepare_subblockchain(data_folder, new_data_folder, start_block, n_blocks):\n",
    "    \"\"\"\n",
    "     Given data in the directory 'data_folder' extract from it a subset of 'n_blocks' blocks\n",
    "     (and TXs associated with these blocks) starting with block 'start_block.\n",
    "     Save the new blockchain in the directory 'new_data_folder' with the same structure\n",
    "     as in 'data_folder'. Basically a vertical reduction of the blockchain.\n",
    "     For now in only handles the files: BH, TX, TXIN, TXOUT.\n",
    "    \"\"\"\n",
    "    print(f\"Trimming {BH_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder, new_data_folder, BH_FILE, start_block, n_blocks, LINE_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TX_FILE}... \")\n",
    "    filter_func_tx = (\n",
    "        lambda line: int(line[1]) >= start_block\n",
    "        and int(line[1]) <= start_block + n_blocks\n",
    "    )\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TX_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx,\n",
    "    )\n",
    "\n",
    "    filtered_txs = load_dat(\n",
    "        path=os.path.join(new_data_folder, TX_FILE), start=0, n_lines=\"all\"\n",
    "    )\n",
    "    filtered_txs_ids = [t[0] for t in filtered_txs]\n",
    "    filter_func_tx_full = lambda line: line[0] in filtered_txs_ids\n",
    "\n",
    "    print(f\"Trimming {TXOUT_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXOUT_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TXIN_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXIN_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def _handle_single_file(\n",
    "    old_dir,\n",
    "    new_dir,\n",
    "    file_name,\n",
    "    start_block,\n",
    "    n_blocks,\n",
    "    batch_size,\n",
    "    filter_func=lambda x: True,\n",
    "):\n",
    "    n_chunks = n_blocks // batch_size\n",
    "    for j in range(n_chunks):\n",
    "        _read_and_append_to_new(\n",
    "            old_dir,\n",
    "            new_dir,\n",
    "            file_name,\n",
    "            start_block + j * batch_size,\n",
    "            batch_size,\n",
    "            filter_func,\n",
    "        )\n",
    "    leftover = n_blocks % batch_size\n",
    "    _read_and_append_to_new(\n",
    "        old_dir, new_dir, file_name, start_block + j * batch_size, leftover, filter_func\n",
    "    )\n",
    "\n",
    "\n",
    "def _read_and_append_to_new(old_dir, new_dir, file_name, start, n_lines, filter_func):\n",
    "    content = load_dat(os.path.join(old_dir, file_name), start=start, n_lines=n_lines)\n",
    "    with open(os.path.join(new_dir, file_name), \"a+\") as fh:\n",
    "        for line in content:\n",
    "            if filter_func(line):\n",
    "                fh.write(\" \".join(list(line)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', '000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f', '1231006505', '1'), ('1', '00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048', '1231469665', '1'), ('2', '000000006a625f06636b8bb6ac7b960a8d03705d1ace08b1a19da3fdcc99ddbd', '1231469744', '1'), ('3', '0000000082b5015589a3fdf2d4baff403e6f0be035a5d9742c1cae6295464449', '1231470173', '1'), ('4', '000000004ebadb55ee9096c9a2f8880e09da59c0d68b1c228da88e48844a1485', '1231470988', '1')]\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "lines = load_dat(\"../btc_data/bh.dat\")\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
