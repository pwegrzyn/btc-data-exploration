{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Bitcoin database\n",
    "\n",
    "\n",
    "## Analyzing the transactions dataset\n",
    "The first step after acquiring the dataset from the Bitcoin DB was to properly reduce it's size and transform it to a nice dataset (as a numpy data array). This was achieved by dumping the .dat files produced by the modified Bitcoin client (see https://github.com/dkondor/bitcoin/tree/0.16 for more info) to a database engine, and then querying it to extract the reduced data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query used to create the transaction dataset:\n",
    "```sql\n",
    "select\n",
    "  tx.txID as id,\n",
    "  bh.block_timestamp as timestamp,\n",
    "  addresses.address as src_addr,\n",
    "  tx.n_inputs,\n",
    "  tx.n_outputs,\n",
    "  sum(txin.sum) as inputs_sum,\n",
    "  sum(txout.sum) as outputs_sum,\n",
    "  min(txin.sum) as min_input,\n",
    "  max(txin.sum) as max_input,\n",
    "  min(txout.sum) as min_ouput,\n",
    "  max(txout.sum) as max_output\n",
    "from\n",
    "  tx\n",
    "  join bh on tx.blockID = bh.blockID\n",
    "  join txin on tx.txID = txin.txID\n",
    "  join txout on tx.txID = txout.txID\n",
    "  join addresses on txin.addrID = addresses.addrID\n",
    "group BY\n",
    "  tx.txID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load the data and prepare it in some way\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BH_FILE = \"bh.dat\"\n",
    "TX_FILE = \"tx.dat\"\n",
    "TXIN_FILE = \"txin.dat\"\n",
    "TXOUT_FILE = \"txout.dat\"\n",
    "\n",
    "LINE_BATCH_SIZE = 500\n",
    "\n",
    "\n",
    "def load_dat(path, as_ndarray=False, cast_to=None, start=0, n_lines=\"all\"):\n",
    "    \"\"\"\n",
    "     Read 'n_lines' lines starting from line 'start' from the file 'path'.\n",
    "     Return numpy array or list of tuples.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    with open(path, \"r\") as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            if i >= start and (n_lines == \"all\" or n_lines > 0):\n",
    "                split_line = line.split()\n",
    "                cast_line = map(cast_to, split_line) if cast_to is not None else split_line\n",
    "                result_list.append(tuple(cast_line))\n",
    "                if type(n_lines) is not str:\n",
    "                    n_lines -= 1\n",
    "    return np.array(result_list) if as_ndarray else result_list\n",
    "\n",
    "\n",
    "def prepare_subblockchain(data_folder, new_data_folder, start_block, n_blocks):\n",
    "    \"\"\"\n",
    "     Given data in the directory 'data_folder' extract from it a subset of 'n_blocks' blocks\n",
    "     (and TXs associated with these blocks) starting with block 'start_block.\n",
    "     Save the new blockchain in the directory 'new_data_folder' with the same structure\n",
    "     as in 'data_folder'. Basically a vertical reduction of the blockchain.\n",
    "     For now in only handles the files: BH, TX, TXIN, TXOUT.\n",
    "    \"\"\"\n",
    "    print(f\"Trimming {BH_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder, new_data_folder, BH_FILE, start_block, n_blocks, LINE_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TX_FILE}... \")\n",
    "    filter_func_tx = (\n",
    "        lambda line: int(line[1]) >= start_block\n",
    "        and int(line[1]) <= start_block + n_blocks\n",
    "    )\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TX_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx,\n",
    "    )\n",
    "\n",
    "    filtered_txs = load_dat(\n",
    "        path=os.path.join(new_data_folder, TX_FILE), start=0, n_lines=\"all\"\n",
    "    )\n",
    "    filtered_txs_ids = [t[0] for t in filtered_txs]\n",
    "    filter_func_tx_full = lambda line: line[0] in filtered_txs_ids\n",
    "\n",
    "    print(f\"Trimming {TXOUT_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXOUT_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TXIN_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXIN_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def _handle_single_file(\n",
    "    old_dir,\n",
    "    new_dir,\n",
    "    file_name,\n",
    "    start_block,\n",
    "    n_blocks,\n",
    "    batch_size,\n",
    "    filter_func=lambda x: True,\n",
    "):\n",
    "    n_chunks = n_blocks // batch_size\n",
    "    for j in range(n_chunks):\n",
    "        _read_and_append_to_new(\n",
    "            old_dir,\n",
    "            new_dir,\n",
    "            file_name,\n",
    "            start_block + j * batch_size,\n",
    "            batch_size,\n",
    "            filter_func,\n",
    "        )\n",
    "    leftover = n_blocks % batch_size\n",
    "    _read_and_append_to_new(\n",
    "        old_dir, new_dir, file_name, start_block + n_chunks * batch_size, leftover, filter_func\n",
    "    )\n",
    "\n",
    "\n",
    "def _read_and_append_to_new(old_dir, new_dir, file_name, start, n_lines, filter_func):\n",
    "    content = load_dat(os.path.join(old_dir, file_name), start=start, n_lines=n_lines)\n",
    "    with open(os.path.join(new_dir, file_name), \"a+\") as fh:\n",
    "        for line in content:\n",
    "            if filter_func(line):\n",
    "                fh.write(\" \".join(list(line)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction dataset shape: (1642765, 11)\n",
      "Memory used by the transactions dataset: 72281660 bytes\n",
      "Data type of the transactions dataset: float32\n",
      "Column names of the transactions dataset:\n",
      " ('id', 'timestamp', 'src_addr', 'n_inputs', 'n_outputs', 'inputs_sum', 'outputs_sum', 'min_input', 'max_input', 'min_ouput', 'max_output')\n"
     ]
    }
   ],
   "source": [
    "# Loading the transactions data\n",
    "\n",
    "TX_DATASET_PATH = \"../btc_data/transactions.txt\"\n",
    "\n",
    "tx_dataset = load_dat(TX_DATASET_PATH, as_ndarray=True, start=2, cast_to=np.float32)\n",
    "print(f\"Transaction dataset shape: {tx_dataset.shape}\")\n",
    "print(f\"Memory used by the transactions dataset: {tx_dataset.nbytes} bytes\")\n",
    "print(f\"Data type of the transactions dataset: {tx_dataset.dtype}\")\n",
    "\n",
    "tx_dataset_cols = load_dat(TX_DATASET_PATH, n_lines=1)[0]\n",
    "print(\"Column names of the transactions dataset:\\n\", tx_dataset_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the transactions dataset:\n",
    "The dataset contains 1 642 765 last transaction created on the Bitcoin network as of Feb 2018. The columns present in the dataset are as follows:\n",
    "* **id** - the ID of a given transaction\n",
    "* **timestamp** - Unix timestamp of when the transaction has been issued (closly tied to the time of creation of a the block in which this transaction has been added)\n",
    "* **src_addr** - represents the address (based on the public key) of the user who created this transaction\n",
    "* **n_inputs** - number of input transactions attached to this transaction\n",
    "* **n_outputs** - number of output transactions defined by this transaction\n",
    "* **inputs_sum** - the total sum of currency value for the input transactions to this transaction\n",
    "* **outputs_sum** - the total sum of currency value for the output transaction o this transaction\n",
    "* **min_input** - the miniaml currency value of an input to this transaction\n",
    "* **max_input** - the maximal currency value of an input to this transaction\n",
    "* **min_output** - the minimal currencly value of an output defined by this transaction\n",
    "* **max_output** - the maximal currency value of an output defined by this transaction\n",
    "\n",
    "*All currency values are to be interpreted as multiplies of Satoshis (1e-8 BTC)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below of some examples of the values of the given transaction fields\n",
    "\n",
    "id          timestamp   src_addr    n_inputs    n_outputs   inputs_sum   outputs_sum  min_input   max_input   min_ouput   max_output\n",
    "----------  ----------  ----------  ----------  ----------  -----------  -----------  ----------  ----------  ----------  ----------\n",
    "296681117   1517425124  58771719    1           2           11393820000  5696310000   5696910000  5696910000  55950000    5640360000\n",
    "296681118   1517425124  305440412   1           2           87730398     43685199     43865199    43865199    8685199     35000000  \n",
    "296681119   1517425124  367858480   2           2           118932776    118371776    26342372    33124016    1792388     57393500  \n",
    "296681120   1517425124  39848381    1           3           608190       2730         202730      202730      0           2184      \n",
    "296681121   1517425124  39848381    1           3           608190       2730         202730      202730      0           2184      \n",
    "296681122   1517425124  356565275   2           2           8147256      7637256      50328       4023300     51109       3767519   \n",
    "296681123   1517425124  341821220   1           2           87582844     43677686     43791422    43791422    1322600     42355086  \n",
    "296681124   1517425124  367841997   1           2           269005064    134352532    134502532   134502532   1160401     133192131 \n",
    "296681125   1517425124  117187599   1           2           1000000000   499850000    500000000   500000000   68453       499781547 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important step of the analysis was to make sure that the data is in the proper\n",
    "# type, that there are no Null values and the the data is semantically correct\n",
    "# We also want to make sure that some columns (like n_inputs or n_outputs) are not negative.\n",
    "\n",
    "\n",
    "def ensure_cols_non_negative(dataset, cols):\n",
    "    for index in cols:\n",
    "        dataset_slice = dataset[:, index]\n",
    "        negatives = dataset_slice[dataset_slice < 0]\n",
    "        if len(negatives) > 0:\n",
    "            raise ValueError(f\"Column {index} contained a negative value!\")\n",
    "\n",
    "\n",
    "def ensure_cols_not_nan(dataset, cols):\n",
    "    for index in cols:\n",
    "        dataset_slice = dataset[:, index]\n",
    "        nans = dataset_slice[dataset_slice == np.NaN]\n",
    "        if len(nans) > 0:\n",
    "            raise ValueError(f\"Column {index} contained a NaN value!\")\n",
    "\n",
    "\n",
    "ensure_cols_non_negative(tx_dataset, [1, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "ensure_cols_not_nan(tx_dataset, range(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that PCA works well in a reasonable amount of time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed_tx_dataset = pca.fit_transform(tx_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
