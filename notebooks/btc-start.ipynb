{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Bitcoin database\n",
    "\n",
    "\n",
    "## Analyzing the transactions dataset\n",
    "The first step after acquiring the dataset from the Bitcoin DB was to properly reduce it's size and transform it to a nice dataset (as a numpy data array). This was achieved by dumping the .dat files produced by the modified Bitcoin client (see https://github.com/dkondor/bitcoin/tree/0.16 for more info) to a database engine, and then querying it to extract the reduced data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query used to create the transaction dataset:\n",
    "```sql\n",
    "select\n",
    "  tx.txID as id,\n",
    "  bh.block_timestamp as timestamp,\n",
    "  addresses.address as src_addr,\n",
    "  tx.n_inputs,\n",
    "  tx.n_outputs,\n",
    "  sum(txin.sum) as inputs_sum,\n",
    "  sum(txout.sum) as outputs_sum,\n",
    "  min(txin.sum) as min_input,\n",
    "  max(txin.sum) as max_input,\n",
    "  min(txout.sum) as min_ouput,\n",
    "  max(txout.sum) as max_output\n",
    "from\n",
    "  tx\n",
    "  join bh on tx.blockID = bh.blockID\n",
    "  join txin on tx.txID = txin.txID\n",
    "  join txout on tx.txID = txout.txID\n",
    "  join addresses on txin.addrID = addresses.addrID\n",
    "group BY\n",
    "  tx.txID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load the data and prepare it in some way\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BH_FILE = \"bh.dat\"\n",
    "TX_FILE = \"tx.dat\"\n",
    "TXIN_FILE = \"txin.dat\"\n",
    "TXOUT_FILE = \"txout.dat\"\n",
    "\n",
    "LINE_BATCH_SIZE = 500\n",
    "\n",
    "\n",
    "def load_dat(path, as_ndarray=False, cast_to=None, start=0, n_lines=\"all\"):\n",
    "    \"\"\"\n",
    "     Read 'n_lines' lines starting from line 'start' from the file 'path'.\n",
    "     Return numpy array or list of tuples.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    with open(path, \"r\") as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            if i >= start and (n_lines == \"all\" or n_lines > 0):\n",
    "                split_line = line.split()\n",
    "                cast_line = map(cast_to, split_line) if cast_to is not None else split_line\n",
    "                result_list.append(tuple(cast_line))\n",
    "                if type(n_lines) is not str:\n",
    "                    n_lines -= 1\n",
    "    return np.array(result_list) if as_ndarray else result_list\n",
    "\n",
    "\n",
    "def prepare_subblockchain(data_folder, new_data_folder, start_block, n_blocks):\n",
    "    \"\"\"\n",
    "     Given data in the directory 'data_folder' extract from it a subset of 'n_blocks' blocks\n",
    "     (and TXs associated with these blocks) starting with block 'start_block.\n",
    "     Save the new blockchain in the directory 'new_data_folder' with the same structure\n",
    "     as in 'data_folder'. Basically a vertical reduction of the blockchain.\n",
    "     For now in only handles the files: BH, TX, TXIN, TXOUT.\n",
    "    \"\"\"\n",
    "    print(f\"Trimming {BH_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder, new_data_folder, BH_FILE, start_block, n_blocks, LINE_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TX_FILE}... \")\n",
    "    filter_func_tx = (\n",
    "        lambda line: int(line[1]) >= start_block\n",
    "        and int(line[1]) <= start_block + n_blocks\n",
    "    )\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TX_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx,\n",
    "    )\n",
    "\n",
    "    filtered_txs = load_dat(\n",
    "        path=os.path.join(new_data_folder, TX_FILE), start=0, n_lines=\"all\"\n",
    "    )\n",
    "    filtered_txs_ids = [t[0] for t in filtered_txs]\n",
    "    filter_func_tx_full = lambda line: line[0] in filtered_txs_ids\n",
    "\n",
    "    print(f\"Trimming {TXOUT_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXOUT_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "\n",
    "    print(f\"Trimming {TXIN_FILE}... \")\n",
    "    _handle_single_file(\n",
    "        data_folder,\n",
    "        new_data_folder,\n",
    "        TXIN_FILE,\n",
    "        start_block,\n",
    "        n_blocks,\n",
    "        LINE_BATCH_SIZE,\n",
    "        filter_func=filter_func_tx_full,\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def _handle_single_file(\n",
    "    old_dir,\n",
    "    new_dir,\n",
    "    file_name,\n",
    "    start_block,\n",
    "    n_blocks,\n",
    "    batch_size,\n",
    "    filter_func=lambda x: True,\n",
    "):\n",
    "    n_chunks = n_blocks // batch_size\n",
    "    for j in range(n_chunks):\n",
    "        _read_and_append_to_new(\n",
    "            old_dir,\n",
    "            new_dir,\n",
    "            file_name,\n",
    "            start_block + j * batch_size,\n",
    "            batch_size,\n",
    "            filter_func,\n",
    "        )\n",
    "    leftover = n_blocks % batch_size\n",
    "    _read_and_append_to_new(\n",
    "        old_dir, new_dir, file_name, start_block + n_chunks * batch_size, leftover, filter_func\n",
    "    )\n",
    "\n",
    "\n",
    "def _read_and_append_to_new(old_dir, new_dir, file_name, start, n_lines, filter_func):\n",
    "    content = load_dat(os.path.join(old_dir, file_name), start=start, n_lines=n_lines)\n",
    "    with open(os.path.join(new_dir, file_name), \"a+\") as fh:\n",
    "        for line in content:\n",
    "            if filter_func(line):\n",
    "                fh.write(\" \".join(list(line)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction dataset shape: (1642765, 11)\n",
      "Memory used by the transactions dataset: 72281660 bytes\n",
      "Data type of the transactions dataset: float32\n",
      "Column names of the transactions dataset:\n",
      " ('id', 'timestamp', 'src_addr', 'n_inputs', 'n_outputs', 'inputs_sum', 'outputs_sum', 'min_input', 'max_input', 'min_ouput', 'max_output')\n"
     ]
    }
   ],
   "source": [
    "# Loading the transactions data\n",
    "\n",
    "TX_DATASET_PATH = \"../btc_data/transactions.txt\"\n",
    "\n",
    "tx_dataset = load_dat(TX_DATASET_PATH, as_ndarray=True, start=2, cast_to=np.float32)\n",
    "print(f\"Transaction dataset shape: {tx_dataset.shape}\")\n",
    "print(f\"Memory used by the transactions dataset: {tx_dataset.nbytes} bytes\")\n",
    "print(f\"Data type of the transactions dataset: {tx_dataset.dtype}\")\n",
    "\n",
    "tx_dataset_cols = load_dat(TX_DATASET_PATH, n_lines=1)[0]\n",
    "print(\"Column names of the transactions dataset:\\n\", tx_dataset_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the transactions dataset:\n",
    "The dataset contains 1 642 765 last transaction created on the Bitcoin network as of Feb 2018. The columns present in the dataset are as follows:\n",
    "* **id** - the ID of a given transaction\n",
    "* **timestamp** - Unix timestamp of when the transaction has been issued (closly tied to the time of creation of a the block in which this transaction has been added)\n",
    "* **src_addr** - represents the address (based on the public key) of the user who created this transaction\n",
    "* **n_inputs** - number of input transactions attached to this transaction\n",
    "* **n_outputs** - number of output transactions defined by this transaction\n",
    "* **inputs_sum** - the total sum of currency value for the input transactions to this transaction\n",
    "* **outputs_sum** - the total sum of currency value for the output transaction o this transaction\n",
    "* **min_input** - the miniaml currency value of an input to this transaction\n",
    "* **max_input** - the maximal currency value of an input to this transaction\n",
    "* **min_output** - the minimal currencly value of an output defined by this transaction\n",
    "* **max_output** - the maximal currency value of an output defined by this transaction\n",
    "\n",
    "*All currency values are to be interpreted as multiplies of Satoshis (1e-8 BTC)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below of some examples of the values of the given transaction fields\n",
    "\n",
    "id          timestamp   src_addr    n_inputs    n_outputs   inputs_sum   outputs_sum  min_input   max_input   min_ouput   max_output\n",
    "----------  ----------  ----------  ----------  ----------  -----------  -----------  ----------  ----------  ----------  ----------\n",
    "296681117   1517425124  58771719    1           2           11393820000  5696310000   5696910000  5696910000  55950000    5640360000\n",
    "296681118   1517425124  305440412   1           2           87730398     43685199     43865199    43865199    8685199     35000000  \n",
    "296681119   1517425124  367858480   2           2           118932776    118371776    26342372    33124016    1792388     57393500  \n",
    "296681120   1517425124  39848381    1           3           608190       2730         202730      202730      0           2184      \n",
    "296681121   1517425124  39848381    1           3           608190       2730         202730      202730      0           2184      \n",
    "296681122   1517425124  356565275   2           2           8147256      7637256      50328       4023300     51109       3767519   \n",
    "296681123   1517425124  341821220   1           2           87582844     43677686     43791422    43791422    1322600     42355086  \n",
    "296681124   1517425124  367841997   1           2           269005064    134352532    134502532   134502532   1160401     133192131 \n",
    "296681125   1517425124  117187599   1           2           1000000000   499850000    500000000   500000000   68453       499781547 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important step of the analysis was to make sure that the data is in the proper\n",
    "# type, that there are no Null values and the the data is semantically correct\n",
    "# We also want to make sure that some columns (like n_inputs or n_outputs) are not negative.\n",
    "\n",
    "\n",
    "def ensure_cols_non_negative(dataset, cols):\n",
    "    for index in cols:\n",
    "        dataset_slice = dataset[:, index]\n",
    "        negatives = dataset_slice[dataset_slice < 0]\n",
    "        if len(negatives) > 0:\n",
    "            raise ValueError(f\"Column {index} contained a negative value!\")\n",
    "\n",
    "\n",
    "def ensure_cols_not_nan(dataset, cols):\n",
    "    for index in cols:\n",
    "        dataset_slice = dataset[:, index]\n",
    "        nans = dataset_slice[dataset_slice == np.NaN]\n",
    "        if len(nans) > 0:\n",
    "            raise ValueError(f\"Column {index} contained a NaN value!\")\n",
    "\n",
    "\n",
    "ensure_cols_non_negative(tx_dataset, [1, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "ensure_cols_not_nan(tx_dataset, range(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając pewność, że dane są w odpowiednim formacie oraz brak w nich niedopuszczalnych wartości możemy przejść do faktycznej części związanej z eksploracją. W pierwszym kroku policzone zostaną podstawowe statystyki zebranego zbioru danych (**średnie, odchylenia standardowe**, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/.pyenv/versions/3.7.6/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>src_addr</th>\n",
       "      <th>n_inputs</th>\n",
       "      <th>n_outputs</th>\n",
       "      <th>inputs_sum</th>\n",
       "      <th>outputs_sum</th>\n",
       "      <th>min_input</th>\n",
       "      <th>max_input</th>\n",
       "      <th>min_ouput</th>\n",
       "      <th>max_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1642765.0</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "      <td>1.642765e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.975028e+08</td>\n",
       "      <td>1.517750e+09</td>\n",
       "      <td>337005920.0</td>\n",
       "      <td>3.828510e+00</td>\n",
       "      <td>2.809052e+00</td>\n",
       "      <td>1.251236e+09</td>\n",
       "      <td>1.084558e+09</td>\n",
       "      <td>2.107748e+08</td>\n",
       "      <td>2.501226e+08</td>\n",
       "      <td>3.168299e+07</td>\n",
       "      <td>2.521970e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.746007e+05</td>\n",
       "      <td>1.927843e+05</td>\n",
       "      <td>70769608.0</td>\n",
       "      <td>2.225318e+01</td>\n",
       "      <td>1.844658e+01</td>\n",
       "      <td>6.212978e+09</td>\n",
       "      <td>5.959210e+09</td>\n",
       "      <td>8.275816e+08</td>\n",
       "      <td>8.978593e+08</td>\n",
       "      <td>2.671221e+08</td>\n",
       "      <td>9.023756e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.966811e+08</td>\n",
       "      <td>1.517425e+09</td>\n",
       "      <td>76316.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.970921e+08</td>\n",
       "      <td>1.517577e+09</td>\n",
       "      <td>341471360.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.358684e+06</td>\n",
       "      <td>2.334742e+06</td>\n",
       "      <td>6.860410e+05</td>\n",
       "      <td>1.503376e+06</td>\n",
       "      <td>1.210000e+05</td>\n",
       "      <td>1.472375e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.975031e+08</td>\n",
       "      <td>1.517758e+09</td>\n",
       "      <td>368037376.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.691864e+07</td>\n",
       "      <td>1.915268e+07</td>\n",
       "      <td>5.522000e+06</td>\n",
       "      <td>1.132616e+07</td>\n",
       "      <td>9.443740e+05</td>\n",
       "      <td>1.105079e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.979141e+08</td>\n",
       "      <td>1.517921e+09</td>\n",
       "      <td>369075712.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.934057e+08</td>\n",
       "      <td>1.282015e+08</td>\n",
       "      <td>5.768067e+07</td>\n",
       "      <td>8.312874e+07</td>\n",
       "      <td>4.605574e+06</td>\n",
       "      <td>8.203275e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.983251e+08</td>\n",
       "      <td>1.518087e+09</td>\n",
       "      <td>370269472.0</td>\n",
       "      <td>1.099000e+03</td>\n",
       "      <td>3.351000e+03</td>\n",
       "      <td>9.999997e+10</td>\n",
       "      <td>9.999997e+10</td>\n",
       "      <td>9.999999e+09</td>\n",
       "      <td>9.999999e+09</td>\n",
       "      <td>9.999999e+09</td>\n",
       "      <td>9.999999e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     timestamp     src_addr      n_inputs     n_outputs  \\\n",
       "count  1.642765e+06  1.642765e+06    1642765.0  1.642765e+06  1.642765e+06   \n",
       "mean   2.975028e+08  1.517750e+09  337005920.0  3.828510e+00  2.809052e+00   \n",
       "std    4.746007e+05  1.927843e+05   70769608.0  2.225318e+01  1.844658e+01   \n",
       "min    2.966811e+08  1.517425e+09      76316.0  1.000000e+00  1.000000e+00   \n",
       "25%    2.970921e+08  1.517577e+09  341471360.0  1.000000e+00  2.000000e+00   \n",
       "50%    2.975031e+08  1.517758e+09  368037376.0  1.000000e+00  2.000000e+00   \n",
       "75%    2.979141e+08  1.517921e+09  369075712.0  1.000000e+00  2.000000e+00   \n",
       "max    2.983251e+08  1.518087e+09  370269472.0  1.099000e+03  3.351000e+03   \n",
       "\n",
       "         inputs_sum   outputs_sum     min_input     max_input     min_ouput  \\\n",
       "count  1.642765e+06  1.642765e+06  1.642765e+06  1.642765e+06  1.642765e+06   \n",
       "mean   1.251236e+09  1.084558e+09  2.107748e+08  2.501226e+08  3.168299e+07   \n",
       "std    6.212978e+09  5.959210e+09  8.275816e+08  8.978593e+08  2.671221e+08   \n",
       "min    3.000000e+02  0.000000e+00  1.000000e+00  3.000000e+02  0.000000e+00   \n",
       "25%    3.358684e+06  2.334742e+06  6.860410e+05  1.503376e+06  1.210000e+05   \n",
       "50%    2.691864e+07  1.915268e+07  5.522000e+06  1.132616e+07  9.443740e+05   \n",
       "75%    1.934057e+08  1.282015e+08  5.768067e+07  8.312874e+07  4.605574e+06   \n",
       "max    9.999997e+10  9.999997e+10  9.999999e+09  9.999999e+09  9.999999e+09   \n",
       "\n",
       "         max_output  \n",
       "count  1.642765e+06  \n",
       "mean   2.521970e+08  \n",
       "std    9.023756e+08  \n",
       "min    0.000000e+00  \n",
       "25%    1.472375e+06  \n",
       "50%    1.105079e+07  \n",
       "75%    8.203275e+07  \n",
       "max    9.999999e+09  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tx_dataframe = pd.DataFrame(data=tx_dataset, columns=tx_dataset_cols)\n",
    "tx_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie statystyk\n",
    "Jedną z pierwszych rzeczy, która rzuca się w oczy jest fakt, że niektóre z cech posiadają bardzo dużą wariancję. Cech te są powiązane z ilością waluty w obiegu. Przykładowo odchylenie standardowe cechy reprezentujące maksymalną kwotę wyjściową transakcji jest równe ok. 10e8. Wynika to z faktu, iż system walutowy Bitcoina jest dość zfragmentaryzowany - każda transakcja, której suma wejść przekracza sumę wyjść musi w rezultaci de facto tworzyć nową pomocniczą transakcję na swój adres, tak aby reszta z powrotem wróciła na nasze konto. W związku z tym w sieci mogą istnieć transakcje o na prawdę dużych kwotach (liczonych w miliardach satoshi), a jednocześnie istnieć może mnóstwo transakcji o małej wartości wynikających z w/w kwoty zwrotnej.\n",
    "\n",
    "Ciekawa jest też analiza cechy src_adddr. Co prawda nie reprezentuje ona żadnej faktycznej wartości liczbowej, jednak i tak może dowiedzieć się ciekawych faktów z analizy jej momentów. I tak przykładowo minimalna wartośc tej cechy to 76316, co oznacza, że w przygotowanym przez nas zbiorze wystąpiły transakcję stworzone przez osobę od bardzo niskim indentyfikatorze, co z kolei oznacza, że osoba ta należała do grupy początkowych użytkoników sieci Bitcoin.\n",
    "\n",
    "Warto również zaznaczyć, że maksymlna wartość pola srd_addr to ponad 370 mln. - możemy to interpretować jako fakt, iż w sieci Bitcoin istnieje obecnie taka właśnie ilość adresów (nie jest to jednak jednoznaczne z liczbą fizycznych użytnikowników, których jest na pewno zdecydowanie mniej z uwagi na fakt, że obecnie zaleca się aby każdą transakcję wykonywać z osobnego adresu publicznego, poza tym fragmentaryzacja majątku na różne adresy jest dobą praktyką jeśli chodzi o bezpieczeństwo).\n",
    "\n",
    "Jedną z najistotniejszych statystyka w kontekście zrozumienia dynamiki całej sieci jest na pewno średnia ilość wejść do transakcji (i analogicznie - średnia ilość wyjść). W przypadku analizowanego przez nas wycinka czasowego transkacji średnia ilość wejść wyniosła 3.83, zaś średnia ilość wyjść niewiele mniej, bo 2.81. Średnią ilość wyjść można dość łatwo zainterpretować: często transakcja \"powołuje\" się na tyle wejść, że te nie sumują się dokładnie do jej wartości, wtedy do powstałej reszty tworzone jest nowe wyjść na adres twórcy. Mechaznim ten pozwala sugerować, że średnia ta powinna być nieco mniejsza niż 2.0, gdyż zdarzają się transakcje, których suma wejść jest dokładnie równa sumie wyjść. Jednak należy pamiętać, że w systemie możliwe są również bardziej rozbudowane transakcję wysyłające środki do wielu różnych odbiorców - to właśnie powoduje, że średnia ilość wyjść jest wyraźnie większa niż 2.0. Znacznie cięższa do interpretacji jest wartość średniej ilości wejść. W tym przypadku możemy jedynie postawić hipotezę, że wspomniana wcześniej fragmenryzacja zbioru transakcji faktycznie występuje, ale nie jest aż tak intensywna jak możnaby na początku zakładac."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza PCA\n",
    "W następnym kroku przeprowadzone zostanie analiza głównych składowych zbioru transakcji. Posłużymy się w tym celu klasą PCA z biblioteki sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed_tx_dataset = pca.fit_transform(tx_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
